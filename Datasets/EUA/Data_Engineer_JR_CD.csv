Job_title,Company,State,City,Min_Salary,Max_Salary,unitText,SalaryCurrency,Job_Desc,Industry,Rating,Date_Posted,Valid_until,Job_Type
Junior Data Engineer,N9 IT SOLUTIONS,,,-1,-1,,USD,"Job Title: - Jr. Data EngineerLocation: - 100% RemotePosition: - W2 Employment ( Should work on our W2 )Eligibility: - (Authorized to work anywhere in the USA and for only those who are staying in the USA)Duration: - Long-term contractJob Description: -Bachelor's Degree or Master’s Degree in Computer Science, Mathematics, Statistics.3+years of experience as a DataStage Developer.3+ years of Complex SQL/PL SQL development experience.1+ years of experience in Writing scripts using Unix or Python.Experience working in Agile Framework.2+ years of experience in Migration from the Data Stage to other technologies.Working experience on AWS.Benefits: -Competitive salaryFlexible work schedule & part-time offE-verifiedH1B and GC fillingOn-job technical supportSkill EnhancementGuesthouse facilities are also availableOpportunity to work with Fortune 500 CompaniesThanks, regardsN9 IT SolutionsJob Type: Full-timePay: $40.00 - $50.00 per hourSchedule:8 hour shiftWork Location: Remote",Tecnologías de la información,,2022-08-29T00:00:00,2022-10-16,FULL_TIME
Junior Data Center Engineer,IMC Financial Markets,New York State,New York,-1,-1,,USD,"LIFE AT IMC AS A JUNIOR DATA CENTER ENGINEER
WHO WE ARE AND WHAT WE DO
IMC is a leading global market maker, using algorithmic trading and advanced technology to buy and sell securities on multiple trading venues worldwide. We provide liquidity to the financial markets, driving efficiencies for buyers and sellers.
Founded in 1989, we are an ambitious, innovative company and identified early on the importance technology would play in the fast-paced evolution of trading. This entrepreneurial spirit still drives us today and can be found in all of our offices around the world.
OUR TEAM
We now operate globally from offices in Europe, the US and Asia Pacific. Our employees work closely together in multidisciplinary teams, making our success possible.
Technology - At IMC, technology is not a department, it is at the heart of everything we do. Our technologists push the limits of possibility, and then look beyond. In our fast-paced environment, short feedback loops mean projects worked on in the morning can enter production the next day.
Trading – Although our traders come from many backgrounds they all have one thing in common: they are at their best solving complex problems. Their insight into global events, market shifts and pricing ensure we are trading in the right place, at the right time.
Business Support - Around the world, IMC’s business support teams are essential for sustaining our success. In our dynamic environment, we have many exciting challenges and multidisciplinary opportunities to shape our operations and make a real impact.
OUR CULTURE
Our employees are our greatest asset so we give them lots of responsibility and the support they need to make a difference. Our flat structure fosters a culture of openness and collaboration, encouraging the sharing of ideas and knowledge. It makes no difference if you have been with us for three days or three years, the best idea wins.
While we work hard, we also have a lot of fun; whether solving complex challenges or in team building, leisure and sporting activities. IMC also enables its employees to contribute towards a better society through our foundation.
We are looking for an entry level to experienced individual to join our team as a Junior Data Center Engineer who will play a critical role in system and application administration of our technical environment. This role will be part of highly experienced team and be provided new challenges on a daily basis. The possibility to make a large impact as part of a dedicated, dynamic team at IMC, both locally and globally, is unlike any other. Our flat culture encourages not only knowledge and best practice sharing, but also the opportunity to have your voice heard.
Key Accountabilities:
Using a ticket system to track work.
Coordinate, Install, and Troubleshoot equipment.
Racking, Cabling, and Provision equipment.
Responsible for the organization, documentation, and inventory in IMC’s Data Centers.
Responsible for the Logistics in shipping/receiving.
Proactively suggesting alternatives for improvements in the IMC Data Center processes, configuration, or procedures.
General understanding of a Data Center Environment and proper usage of Data Center best practices.
Project Management of Data Center related tasks.
Requirements:
1-3 years working in a Data Center is preferred but not requried
Experience with installation, testing, and cleaning of Fiber Optic cabling
Strong documentation skills for creating Standard Operating Procedures in the Data Center.
A Valid Driver’s License
Travel to suburbs of Chicago and New Jersey area.
Communication skills between team members and external vendors.
Lift objects 60+ pounds
Experience with Data Center Facilities Management tools for Power and Cooling.
Basic Bash and Python knowledge is preferred.
Success Factors:
Highly motivated and organized self-starter that enjoys technology.
Able to work flexible hours when required including evening and weekends
High analytical and interpersonal skills
Knowledge of Network and Server Operating Systems.
Dependable and able to work independently.
Ability to manage competing job responsibilities.
OUR HIRING PROCESS
To set you up for success, you can find our hiring process including tips on applying and interviewing with us on our website. Now it’s up to you! Apply today to start an amazing journey with IMC.",Finanzas,,2022-09-08T00:00:00,2022-10-16,FULL_TIME
Cyber Security Fusion Center / Data Derived Intelligence - Junior Data Engineer- AVP,Citi,Florida,Tampa,-1,-1,,USD,"About Citi:
Citi, the leading global bank, has approximately 200 million customer accounts and does business in more than 160 countries and jurisdictions. Citi provides consumers, corporations, governments, and institutions with a broad range of financial products and services, including consumer banking and credit, corporate and investment banking, securities brokerage, transaction services, and wealth management.
As a bank with a brain and a soul, Citi creates economic value that is systemically responsible and in our clients’ best interests. As a financial institution that touches every region of the world and every sector that shapes your daily life, our Enterprise Operations & Technology teams are charged with a mission that rivals any large tech company. Our technology solutions are the foundations of everything we do from keeping the bank safe, managing global resources, and providing the technical tools our workers need to be successful to designing our digital architecture and ensuring our platforms provide a first-class customer experience. We reimagine client and partner experiences to deliver excellence through secure, reliable, and efficient services.
Our commitment to diversity includes a workforce that represents the clients we serve from all walks of life, backgrounds, and origins. We foster an environment where the best people want to work. We value and demand respect for others, promote individuals based on merit, and ensure opportunities for personal development are widely available to all. Ideal candidates are innovators with well-rounded backgrounds who bring their authentic selves to work and complement our culture of delivering results with pride. If you are a problem solver who seeks passion in your work, come join us. We’ll enable growth and progress together.
As a bank with a brain and a soul, Citi creates economic value that is systemically responsible and, in our clients’, best interests. As a financial institution that touches every region of the world and every sector that shapes your daily life, our Enterprise Operations & Technology teams are charged with a mission that rivals any large tech company. Our technology solutions are the foundations of everything we do. We keep the bank safe and provide the technical tools our workers need to be successful. We design our digital architecture and ensure our platforms provide a first-class customer experience. Our operations teams manage risk, resources, and program management. We focus on enterprise resiliency and business continuity. We develop, coordinate, and execute strategic operational plans. Essentially, Enterprise Operations & Technology re-engineers’ client and partner processes to deliver excellence through secure, reliable, and controlled services.
Trust is part of our DNA at Citi. As such, we take safeguarding our customer data very seriously. The Chief Information Security Office (CISO) is made up of deeply dedicated and talented colleagues who work together to ensure the safety of Citi’s and our clients’ assets and information. We manage information security as an end-to-end program – one with a clear mandate and accountability. Our mission is to continually execute and enhance a global security program that is fully anchored to modern control and security frameworks, fully aligned with the technology of the firm, threat-focused and data-driven, and deeply integrated across all Citi businesses globally.
Being talent-driven, we are focused on attracting, developing, and retaining diverse and inclusive talent with a high technical skill level. As a member of our team, we will provide you with career development opportunities at all stages of your career. Our employees model a passion for protecting Citi and our clients and believe in treating others with dignity and respect.
A strong enterprise Cyber Security Operations (CSO) organization is necessary to enable Citi businesses to operate safely. This organization is responsible for all cyber security operations, including assessment, monitoring and detection, intelligence, and incident response and recovery.
Our commitment to diversity includes a workforce that represents the clients we serve globally from all walks of life, backgrounds, and origins. We foster an environment where the best people want to work. We value and demand respect for others, promote individuals based on merit, and ensure opportunities for personal development are widely available to all. Ideal candidates are innovators with well-rounded backgrounds who bring their authentic selves to work and complement our culture of delivering results with pride. If you are a problem solver who seeks passion in your work, come join us. We’ll enable growth and progress together.
In support of the Cyber Security Fusion Center (CSFC) mission, the Data Derived Intelligence team is responsible for researching, developing, and deploying cybersecurity related big data solutions. As such, staff members develop data science solutions to support the analysis of cyber security data that is gathered from a variety of heterogeneous sources.
In order to achieve the above results, this position will be responsible for supporting the CSFC data lake (big data) operations. Including but not limited to implementing and managing data pipelines, analyzing data, providing statistical and visual interpretations to support stakeholders in making data driven decisions.
Responsibilities and Skills:
Experience working with Hadoop, Splunk or Cloud (AWS) ecosystems
Experience supporting and troubleshooting data engineering/ETL processes in production environments
Knowledge of data engineering concepts and experience with associated tool sets to support data wrangling from across a wide array of disparate data sources
Experience processing large amount of structured and unstructured data
Strong understanding of data management controls
Proficiency designing and developing data pipelines across multiple ecosystems
Work as individual contributor and collaboratively as part of a team of teams
The ability to effectively use data to visualize and communicate findings to senior leadership
Programming expertise, ideally in Python, Spark, Java, and Shell/Bash scripting but not limited to them
Proficiency in any variant of SQL
An inquisitive nature and a sense of satisfaction from solving complicated problems
Familiarity with Machine Learning models and statistical data mining techniques
Knowledge of the Hadoop ecosystem and Big Data technologies (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Oozie, Sqoop)
Experience working in Linux environments (bash, zsh, writing cron jobs, use of systemctl for maintenance, setting of environment variables, etc.).
Data analysis skills, including but not limited to use of Excel, MS Access and business intelligence tools (Tableau, Microsoft PowerBI, Microstrategy) to analyze static data sets
Supports data architecture and modeling design by analyzing client operations, applications, and programming
Academic or working experience in the Cyber domain
Qualifications:
1-3 years of relevant experience
Certifications or willingness to earn within 12 months of joining
Consistently demonstrates clear and concise written and verbal communication
Proven influencing and relationship management skills
Proven analytical skills
Education:
Bachelor’s degree/University degree or equivalent experience
This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.
Excellent organizational, interpersonal, and project management skills
Excellent communication skills both written and oral.
Strong customer and quality-focus is a must.
Self-starter and ability to work in a team environment
Persistence and tenacity in chasing down elusive issues across multiple layers of a complex ecosystem
-
Job Family Group:
Technology
-
Job Family:
Information Security
-
Time Type:
Full time
-
Citi is an equal opportunity and affirmative action employer.
Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.
View the ""EEO is the Law"" poster. View the EEO is the Law Supplement.
View the EEO Policy Statement.
View the Pay Transparency Posting
-
Effective November 1, 2021, Citi requires that all successful applicants for positions located in the United States or Puerto Rico be fully vaccinated against COVID-19 as a condition of employment and provide proof of such vaccination prior to commencement of employment.",Finanzas,,2022-08-30T00:00:00,2022-10-16,FULL_TIME
Jr. Data Engineer (REMOTE),IntelliBridge LLC,District of Columbia,Washington,-1,-1,,USD,"Title: Jr. Data Engineer
Clearance: Public Trust clearance will be applied for upon acceptance
Location: Remote
Overview:
IntelliBridge is an award-winning national security company looking for a Jr. Data Engineer to support our contract with the U. S. Customs and Immigration Service (USCIS) on the Records DevSecOps program.
As a Jr. Data Engineer at IntelliBridge, you will be part of an integrated project team delivering scalable and secure systems. You will perform the full suite of DevSecOps tasks in AWS cloud environment to support the continuous development, deployment, integration, and monitoring of the server and client-side solutions. As a member of a project delivery team, you will collaborate with project and product managers, user experience designers, and business analysts.
As a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.
Clearance:
Public Trust clearance requires a background investigation (BI) and entry on duty (EOD) status. This must be applied for upon acceptance of employment offer. Candidates with existing or recently completed Public Trust background investigations are preferred.
Responsibilities/Duties:
Expected to possess proficiency in manipulating financial data for analytical assessments; be proficient with Microsoft tools, specifically Excel; possess experience with programing languages such as Python, java script, and SQL.
 Required Skills and Qualifications:
Bachelor’s degree is preferred.
Minimum of 2 years of analytical experience, or 3 years of experience with a bachelor’s degree.
Minimum of 2 years SQL programming.
Exceptional communication skills with emphasis on effectively communicating detailed analysis and recommendations.
Strong analytical and problem-solving skills.
Experience working in collaborative sessions, while demonstrating strong verbal communication and presentation skills.
Experience producing varied written documentation products.
Ability to recognize problems and opportunities and evaluate strengths and weaknesses of potential solutions to problems.
Shall also have three (3) years of experience in troubleshooting software which may overlap with the experience requirements above.
Shall have experience working in AWS, software containerization and Agile development processes.
Experience and proficiency with Microsoft Office Suite – Excel, Access, Word, and PowerPoint.
About Us:
IntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.",Gobierno y administración pública,,2022-08-31T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,Calpine Corporation,Texas,Houston,-1,-1,,USD,"Calpine Corporation is America's largest generator of electricity from natural gas and geothermal resources with operations in competitive power markets. Its fleet of 76 power plants in operation and one under construction represents nearly 26,000 megawatts of generation capacity. Through wholesale power operations and its retail businesses, Calpine serves customers in 22 states, Canada and Mexico. Its clean, efficient, modern and flexible fleet uses advanced technologies to generate power in a low-carbon and environmentally responsible manner.
The company was established on the premise that a strong commitment to the environment is inextricably linked to excellence in power generation and corporate responsibility. Since its founding in 1984, Calpine has led the power industry in its unwavering commitment to environmental stewardship. In addition, its renewable geothermal plants use steam generated deep below the earth's surface to produce clean, renewable electricity.
Job Summary (includes but is not limited to the following, other duties may be assigned)
At Calpine IT, we are strategic problem solvers. Our mission is to create world-class technology solutions to support Calpine business, lead in the innovative use of data and emerging technologies to redefine the customer experience, and help ""Calpine's business growth"". To continue building our hard-working IT Team, we are seeking candidates who champion innovation, operate effectively in an agile environment, challenge the status quo, and are empowered to take risks. The Calpine Technology Associates Program is a talent development program sought at recruiting small cohorts of highly skilled early-career professionals.
This Junior Data Engineer is crafted to:
Develop associates in the program to well-rounded technologists.
Infuse our technology teams with fresh, innovative thinking.
Increase collaboration and partnerships across the tech organization.
Involve in suggesting/delivering solutions to solve critical business problems
Job Responsibilities
The Junior Data Engineer will be responsible for creating reports to be consumed by end users using the SSRS and Power BI Platform.
Documentation of all SQL Server objects referenced in code.
Creation of SQL Objects in development environment to allow the job to complete successfully
Apply predefined database security roles to enable jobs to run successfully.
Collaborates with program managers, team members and developers on specific project business objectives to determine database or reporting requirements.
Designs and builds reports and stored procedures in adherence with organizational architectural standards.
Job Requirements
Bachelor’s degree in Computer Science, Information Systems, or related field preferred
0 - 3 years’ experience in a SQL development or Data Analyst Writer role; or a combination of directly related education and experience required.
Proficiency in Microsoft Office Suite, Windows, Microsoft SQL Tools, and SQL Server.
Excellent customer service, problem solving, and communication skills with the ability to effectively identify and translate business user needs, and to convey technical information in a clear and concise manner.
Strong organizational skills with the ability to manage timelines and prioritize workload in a high-pressure, results-oriented environment.
Self-starter with the ability to work independently while supporting a team environment.
Data analysis and mapping skills with strong attention to detail and concern for data accuracy.
Intermediate knowledge of SQL commonly used concepts, practices and procedures related to relational databases.
Additional Calpine Information
Vaccination Information: Calpine requires an individual who is newly hired into this position to be vaccinated for COVID-19 within the first 28 days of employment - if not already vaccinated prior to starting employment. If you have any concerns regarding compliance with this requirement, you will need to discuss your concerns with Calpine’s HR department after a decision has been made about whether or not to make you a conditional offer of employment. Calpine does not require applicants to discuss vaccination status prior to receipt of a conditional offer of employment and complies with all applicable laws requiring reasonable accommodation.
Equal Opportunity Employer of Minorities, Females, Protected Veterans, and Individuals with Disabilities.
Calpine is committed to Equal Employment Opportunity and providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your request to hrrecruitment@calpine.com. Determination on requests for reasonable accommodation are made on case-by-case basis.
Please view Equal Employment Opportunity Posters provided by OFCCP here
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","Energía, minería e infraestructura pública",,2022-08-18T00:00:00,2022-10-16,FULL_TIME
Jr. Data Engineer,MediaAgility,New Jersey,Princeton,-1,-1,,USD,"Jr. Data Engineer
Experience Level

Location
Princeton, NJ, United States Of America Onsite
Skills
python SQL
Qualifications
Bachelors/ Masters degree or equivalent work experience

Job Role
About MediaAgility
MediaAgility is a premier global digital consultancy on a mission to engineer solutions to help innovators succeed. The company is headquartered in Princeton, NJ with a presence in 5 countries. We are committed to -
a. Delivering exceptional services to our clients
b. Building a people-first company
c. Being global, diverse, and inclusive in all our operations
As a premier Google Cloud partner, we take our clients from idea to impact with ‘Strategic Thought Partnership’ and ‘Intelligence Solutions’. We offer a full spectrum of services from Digital Advisory (consulting), Digital Transformation (implementation), and Digital Operations (continuous improvement). We do this through a unique Braintrust engagement model, Agile development, and deep expertise on the latest technologies.
Job Description
Design and Develop end to end streaming and batch data analytics pipelines. From data ingestion, processing, storage, analysis, machine
Understand big-data principles and best practices. Deliver projects in data analytics, machine learning and AI.
Perform code reviews, ensure code quality and encourage a culture of excellence.
Be a front face of the company in front of customers and prospects.
Required Skills
Good SQL Experience
Good in Python OR Java programming.
Should understand Data Analytics and Machine Learning systems.
Experience in Agile Methodologies
Excellent communication skills
Desired Certification:
Google Cloud Certified Professional Data Engineer",Tecnologías de la información,,2022-08-31T00:00:00,2022-10-16,OTHER
"Junior Data Engineer-Buckhead, GA",Cortland,Georgia,Atlanta,-1,-1,,USD,"Intro:

At Cortland, you map the story of your success. We don't adhere to the status quo, we love outside industry perspective, and we thrive on exploring possibilities and reimagining solutions. As an innovative leader in multifamily, our high performance continues to drive exponential growth – and we invite you to join us on our journey towards real estate excellence. With tools and guidance to sharpen your skills, you can forge your own career path, love what you do, and let it show.
Job Overview:

As a Junior Data Engineer, you help build and maintain a state-of-the-art analytic platform, positioning Cortland for a future state of continuous global growth and market entry in which data science and analytics spearhead data-driven strategy across all company verticals and platforms.
Roles You Will Play:

The Researcher
Collaborate to architect and implement end-to-end cloud infrastructure (analytics, compute, databases, DevOps, identity, integration, management, networking, security, and storage)
Leverage technologies such as the Azure BI Stack, BI and GIS visualization platforms, and modern developer tools to execute innovative solutions that facilitate data-driven analysis, automation, and data science
Apply dimensional data modeling to solve business problems
The Data Expert
Analyze, develop, and maintain data pipelines from internal and external sources, utilizing Databricks, Python and Azure Data Factory
Profile and analyze data in designing scalable solutions
Apply and build automated test-driven development, continuous integration/delivery, and version control best practices
The Impact You Can Make
Technical problems have met their match with you – your problem-solving prowess can tackle any issues!
You make everyone’s lives easier by providing them with highly accurate and timely reports, keeping the whole team on the same page.
Since you’re passionate and endlessly curious about technology, there is no aspect of new technology that doesn’t interest you – and you have an uncanny knack for understanding all of it with ease.

Building Blocks of Success:

Requirements:
Degree in Computer Science, Engineering, Mathematics, Statistics or related quantitative field
1+ years of experience in RDBMS systems, data warehousing, advanced SQL Server Analytical development, and sophisticated data analysis
1+ years of experience with Azure Cloud Technologies (Data Factory, PowerShell, Data Lake and Data Lake Analytics)
1+ years of experience in Python
Experience with Data Modeling and ETL tools, Business Intelligence platforms, API Integration, and Object-Oriented Programming (OOP)
Ability to thrive in a cross-functional environment utilizing modern technologies (Python, Git, Jenkins, Octopus Deploy, Tensorflow, Domo, ArcGIS, E/R Studio, RedGate DLM Automation, and other tools)
Experience with messaging/event processing tooling and frameworks such as Azure Event Hub, Kafka, Kinesis
Working knowledge of Azure HDInsight + Spark, Azure Databricks, Azure Stream Analytics
Outro:

At Cortland, we create, reimagine, and manage apartment communities for residents nationwide. Headquartered in Atlanta, GA, we have communities and regional offices all over the country, as well as overseas. From product design and procurement to general contracting and property management, we do it all – to make sure our communities are the perfect setting for living life to its fullest.
Our success is fueled by our belief in a better life – where hospitality is always a given, each detail is worth a second thought, and every open door is a new opportunity to go beyond expectations. We come to work every day to create possibilities for people – possibilities that translate into superior living spaces and experiences designed to inspire our residents, associates, and investors to live a better life focused on what matters most to them.
Cortland is an equal opportunity employer, and we’re proud to support and celebrate diversity in the workplace. We are committed to equal consideration for all qualified applicants regardless of race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, protected veteran status, genetic information, or any other characteristic protected by applicable law. If you have a disability and need an accommodation or assistance with the application process and/or using our website, please email talentresources@cortland.com or call 404.965.3988.
Cortland is a drug-free workplace.
Cortland participates in e-verify to verify the employment status of
all persons hired to work in the United States.",Bienes raíces,,2022-09-07T00:00:00,2022-10-16,FULL_TIME
Junior Scientific Data Engineer,OpenTeams,Massachusetts,Boston,-1,-1,,USD,"Who We Are

OpenTeams is the services marketplace where open source software users can find, vet, and contract with service providers. At OpenTeams we believe in a culture of do-ers, learners, and collaborators. We are looking for people who are motivated, humble, curious, and respectful of others. In order to meet the demands of our high growth business, we are looking for talented individuals to provide insights, solutions, and strategy to our internal leadership team and client partners.
What You Will Do
The Junior Scientific Data Engineer will be responsible for the implementation
of the Data Pipelines and custom data models for new clients. This entails execution
on the project scope to ensure that we meet or exceed client expectations and fulfill their
requirements.

Additional duties include research and prototype of data acquisition strategies for scientific lab
instrumentation along with file parsers for instrument output files (.xlsx, .pdf, .txt, .raw, .fid, many
other vendor binaries). Based on project scope, design and build data models, Python data
pipelines, unit tests, integration tests and utility functions.

Requirements
3+ years experience working within Python and scientific data modelingElasticsearch, science background, or experience with scientific instrumentsExcellent verbal and written communications skills; ability to explain technical information in
non-technical language to drive progress
Bachelor's degree in Computer Science, Statistics, Chemistry, or related field or equivalent
work experience

Additionally an ideal candidate would be:
Passionate about science and building solutions to make the data more accessible to the
end-users.
Intellectually curious: Unwavering drive to learn and know more every day.Able to think creatively about how to solve project risks without reducing quality.A team player who is willing to ""roll up your sleeves"" and do what it takes to make the team
successful.
Experienced in Life Sciences and Pharma workflows, especially with drug design and
development, biologics, automation or new modalities.
Knowledgeable of GxP compliance-related activities such as Good Laboratory
Practices (GLP) is ideal
AWS certification preferred
Why You Should Join

You'll become an important part of a collaborative, remote-first team. We are a passionate and ambitious team, with a proven record of success building multiple companies. We strive to provide a working environment that gives you room to learn and grow. OpenTeams is committed to creating a diverse and inclusive work environment and is proud to be an equal opportunity employer.

We offer competitive compensation, and comprehensive benefits package including:

100% of employee's medical, dental, and vision premiums
Company contributions toward employee HSA's
401K plan
6 weeks of PTO a year (Vacation time is not just encouraged, but celebrated and modeled.)

All qualified applicants will receive equal consideration for recruitment, interviews, employment, training, compensation, promotion, and related activities without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status or any and all other protected classes and in accordance with all applicable laws.",,,2022-09-14T00:00:00,2022-10-16,OTHER
Junior Data Engineer - 22-03044,"Evergreen Technologies, LLC.",Minnesota,Minneapolis,-1,-1,,USD,"Role: Junior Data Engineer

Location: Remote Role

QUALIFICATIONS, SKILLS, AND EXPERIENCE:

Strong organizational and project management skills
Python basic to intermediate
Azure cloud - basic
Databricks basic or ability to quickly learn how to work within Databricks
Jupyter notebooks basic to intermediate
Git version control basic to intermediate
Linux, basic comfortability to work on the command line
Ability to edit JSON files
Good interpersonal and communication skills
1 or more years of experience working within a highly technical team

Key Responsibilities:

Schedule and execute the template jobs required to optimize pricing for each grocery category
Move data files from cloud environment to on-prem servers
Edit existing code, where necessary, to establish pricing specific business rules provided by Client stakeholders
Load data files into a Dash based interactive dashboard used by EPP and Client stakeholders to facilitate pricing optimization
Running batch scripts
Collaborate with 84.51 EPP consultants to appropriately deploy pricing optimization solution within each scheduled grocery category; attend meetings, in-take business requirements, translate requirements to code, align on and complete action items
Adhere to stringent quality assurance and documentation standards (e.g., git, markdown, Confluence)",,,2022-09-09T00:00:00,2022-10-16,OTHER
Data Engineer,"Deutsche Boerse Systems, Inc",Illinois,Chicago,-1,-1,,USD,"Deutsche Börse Group has a vacancy for aJunior Data Warehouse / Big Data Operations EngineerDivision/sectionStatistiX OperationsField of activityStatistiX® is the Data Warehouse of Deutsche Börse Group. StatistiX® provides external and internal users worldwide with statistical information on financial markets. Aim of the position is not only to support the StatistiX DevOps Team of the classical Data Warehouse, but also to support the journey to a state-of-the-art data lake. The job position requires a high interest in IT operations of both, a grown solution and a new technology stack that keeps evolving.This is an exciting opportunity to dive deep into the challenges of modern data ingestion, data processing and data analytics. The position will give insights and hands-on experience in classic data warehouse technologies such as Oracle SQL and Informatica PowerCenter as well as in open-source big data technologies and components such as Spark, Kafka, NoSQL databases, APIs and Notebooks. All of this comes with a fast-paced environment and a highly committed, collaborative and diverse team.Tasks/responsibilitiesGet to know the concept of large-scale Data Warehousing and Data LakesTake over first own responsibilities, such as execution of deploymentsSupport in daily batch processing, e.g. creation and distribution of pre-defined standard reports to Business UnitsReact, analyse, solve and document issues in the automatic processingImplement own suggestions for solutions or optimization of existing processesQualifications/required skillsUniversity degree in «Computer Science» and/or a comparable trainingAn affinity for analyses and problem solvingProficiency in written and spoken English is mandatory; German language skills will be an assetDesirable: first experience with relational or non-relational databases / SQL and/or Linux shellLocation: Chicago, ILInterested?Please follow the link below to apply:⁣Junior Data Warehouse / Big Data Operations Engineer (f/m/d) (Frankfurt am Main) › Deutsche Börse Group (deutsche-boerse.com)Job Type: Full-timeBenefits:401(k)401(k) matchingDental insuranceFlexible spending accountHealth insuranceLife insurancePaid time offParental leaveVision insuranceSchedule:8 hour shiftSupplemental pay types:Bonus payAbility to commute/relocate:Chicago, IL 60606: Reliably commute or planning to relocate before starting work (Required)Experience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",,,2022-09-13T00:00:00,2022-10-16,FULL_TIME
Jr. Data Engineer - Big Data,I28 Technologies,Hawaii,Honolulu,-1,-1,,USD,"Can work as part of the small team with fairly independent roleFlexibility and adaptability to meet deadlines.Self-motivated and team player attributes.Exposure to Cloud technologiesRDBMS technologies such as Oracle, SQL ServerKnowledge in supporting web based application (ideally Software-as-a-Service)Knowledge of working with multi-cultural and geographically disparate teamsJob Type: Full-timeSalary: $55,000.00 - $60,000.00 per yearBenefits:Health insuranceSchedule:8 hour shiftWork Location: One location",Finanzas,,2022-09-16T00:00:00,2022-10-16,FULL_TIME
Jr. Data Engineer,Human Rights Campaign,District of Columbia,Washington,-1,-1,,USD,"We strongly encourage people of color, transgender and non-binary people to apply. HRC is an equal opportunity employer and welcomes everyone, including non-LGBTQ+ people, to join our team. Don’t meet every single requirement? Studies have shown that people from marginalized communities are less likely to apply to jobs unless they meet every single qualification. At HRC, we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.


Position Summary:

The Jr. Data Engineer will assist in the management and utilization of data with the overarching goal of leveraging HRC’s data ecosystem to strategically support and fulfill the organization’s fundraising and programmatic objectives. The Jr. Data Engineer will help ensure the integrity of, enhancements and improvements to, and the management of the data flowing into and out of the data warehouse, which centralizes data from many sources, including HRC’s fundraising database, voter data, and data from communications platforms like email, SMS and more. The Jr. Data Engineer, together with the data team, will serve a pivotal role in the integration and maintenance of all data related to HRC’s members, supporters and contacts.


Position Responsibilities:

Monitor workflows and scripts to make sure data is flowing as expected, and help troubleshoot and solve any problems that may occur
Ensure quality control (QC) of data sets by:
Reviewing and comparing two or more datasets against each other in detail, and helping to resolve any differences in data sets from what is expected
Reviewing data within a dataset, including record lookups in source databases, checking validity of data, etc.
Write and maintain documentation on workflows, processes and integrations
Develop or assist in developing workflows and scripts that manage the integration of data
Extract, transform, and load data (ETL) from a variety of different sources into HRC’s data warehouse and other databases, including understanding data types and ensuring consistency of data across data sets
Understand HRC business rules as they relate to various data sets and workflows
Actively engage in understanding organizational fundraising and programmatic objectives
Maintain internal tools and add new features to internal tools
Assist in fulfilling data requests, performing data extracts and uploads and reviewing team members’ work as part of a robust quality control process
Participate in data team, departmental, and staff meetings as needed
Ensure that all work is documented and assist the team in documentation
Coordinate activity with and serve as back-up for other Data team members
Other duties as assigned.


Position Qualifications:

A bachelor's degree or equivalent work experience.
Three to four years of relevant work experience.
Great attention to detail
Ability to work quickly and accurately with a high level of professionalism, organization and collaboration required.
Ability to work individually, with the department and with other departments at HRC
Ability to work in a fast-paced, dynamic work environment.
Ability to clearly document processes, procedures, and workflows
Computer programming, with particular emphasis on Python
SQL
Understand ""http request"" basics
Salesforce experience or knowledge is helpful
Must be proficient with Microsoft Office applications and Google Apps (Gmail, Google Docs and Drive)
Flexibility with work schedule is required; this position requires some evening and weekend work.
Commitment to LGBTQ+ equality


All positions at the Human Rights Campaign may require travel on a regular basis or periodically. Where the need arises for business travel, appropriate compensation as outlined by the Fair Labor Standards Act will apply.",ONG y Organizaciones sin fines de lucro,,2022-09-14T00:00:00,2022-10-16,OTHER
JUNIOR DATA ENGINEER (100% REMOTE AVAILABLE),University of Washington,Washington State,Seattle,-1,-1,,USD,"Notes: As a UW employee, you will enjoy generous benefits and work/life programs. For a complete description of our benefits for this position, please visit our website, click here.


As a UW employee, you have a unique opportunity to change lives on our campuses, in our state and around the world. UW employees offer their boundless energy, creative problem solving skills and dedication to build stronger minds and a healthier world.


UW faculty and staff also enjoy outstanding benefits, professional growth opportunities and unique resources in an environment noted for diversity, intellectual excitement, artistic pursuits and natural beauty.


This position provides opportunities to work in a rewarding environment and empower our diverse community of faculty, staff, fellows, residents, and students to read, use and act on data as the core decision process of our business. As an employee you will receive excellent benefits, have access to work/life programs and get to work alongside amazing coworkers in our Dean of Medicine IT department.


UW Medicine’s mission is to improve the health of the public by advancing medical knowledge, providing outstanding primary and specialty care to the people of the region, and preparing tomorrow’s physicians, scientists and other health professionals. UW Medicine owns or operates Harborview Medical Center (HMC), Northwest Hospital & Medical Center (NWH), Valley Medical Center (VMC), UW Medical Center (UWMC), a network of UW Medicine Neighborhood Clinics (UWNC) that provide primary care, UW Physicians, UW School of Medicine, Airlift Northwest, and other owned, operated or affiliated entities. In addition, UW Medicine shares in the ownership and governance of Children’s University Medical Group and Seattle Cancer Care Alliance a partnership among UW Medicine, Fred Hutchinson Cancer Research, and Seattle Children’s.


Dean of Medicine IT has an outstanding opportunity for a Junior Data Engineer to join their team.


The Junior Data Engineer (payroll title: Senior Computer Specialist) will be responsible for several aspects of data management to include: research issues and implement corrections, operational support, ETL, TSQL development, application data integration support on multiple business domains (Academic, Finance, Faculty, etc.). The Junior Data Engineer will be part of a newer team supporting existing data processes, integrating and migrating to modern technology. The position will advance our data culture by supporting our business customers, learn and explain data opportunities, further develop, and enhance our data.


POSITION COMPLEXITIES

This position will serve a key role, working with Dean of Medicine or School of Medicine units, vendors, UW Information Technology (UW IT), UW Medicine Information Technology (UW Med ITS), internal business and technical staff to recommend strategies, design and maintain data architecture: internal & external stores, master data management, data acquisition & integration, data delivery platforms, data propagation & distribution, data access & providers, analytic environments, predictive modeling environments, database management systems and metadata repository & services.


The successful candidate will be responsible for supporting and enhancing these integrated systems and balance multiple competing priorities, clearly setting expectations while maintaining excellent customer relationships.


DUTIES AND RESPONSIBILITIES

Develop and maintain databases, SQL Jobs, incident resolution, ETL, TSQL development, application data integration support, and assist with business intelligence solutions.

Research, strategize, and write proposals to advance existing systems and manage new requirements.

Provide best practices and guidance on all aspects of data systems to include: future plans, products, security, privacy compliance, technology, training, design, implementation, support, vendor and customer relationships, future cloud options, ETL tools, and research trends.

Provide DBA support as needed for backup/recovery, related tools, upgrades, migrations, connections, access and permissions when applicable.

Foster and maintain excellent relationships with a wide variety of internal, external business and technical partners.

Attend unit-wide meetings; advance technical and professional skills through continuing education, mentoring programs or personal study; and research and maintain knowledge of new technologies for departmental use.


MINIMUM REQUIREMENTS

Bachelor’s degree in Computer Science, Information Management, related field or equivalent experience.

Two years related experience.


Equivalent education/experience will substitute for all minimum qualifications except when there are legal requirements, such as a license/certification/registration.


ADDITIONAL REQUIREMENTS

SQL development experience using Transact SQL.

Experience with an ETL technology (ex. SSIS).

Strong problem-solving skills, investigating and resolving issues.

Demonstrated ability to work with minimum supervision as an individual contributor to projects and daily work.

Strong sense of focus and attention to detail, flexibility, self-motivation, organizational skills, excellent written/oral communication skills, and ability to adapt to a range of technical and non-technical audiences.


DESIRED QUALIFICATIONS

Experience working in higher education, research, or health services organizations.

Experience working in virtual servers, containers, or cloud computing (such as Azure, AWS, or serverless computing) environments.

Experience working with reporting technologies such as Crystal, Business Objects, Tableau, Power BI, or Reporting Services.

Some experience with Java, C#, Python, R script or M query.

An understanding of contemporary IT systems, network infrastructure and their interdependencies.

Experience with data cleansing and/or ensuring data integrity.

Experience working with HIPAA, FERPA and GDPR data.


Application Process:

The application process for UW positions may include completion of a variety of online assessments to obtain additional information that will be used in the evaluation process. These assessments may include Work Authorization, Cover Letter and/or others. Any assessments that you need to complete will appear on your screen as soon as you select “Apply to this position”. Once you begin an assessment, it must be completed at that time; if you do not complete the assessment you will be prompted to do so the next time you access your “My Jobs” page. If you select to take it later, it will appear on your ""My Jobs"" page to take when you are ready. Please note that your application will not be reviewed, and you will not be considered for this position until all required assessments have been completed.


Applicants considered for this position will be required to disclose if they are the subject of any substantiated findings or current investigations related to sexual misconduct at their current employment and past employment. Disclosure is required under Washington state law.

Committed to attracting and retaining a diverse staff, the University of Washington will honor your experiences, perspectives and unique identity. Together, our community strives to create and maintain working and learning environments that are inclusive, equitable and welcoming.
The University of Washington is a leader in environmental stewardship & sustainability, and committed to becoming climate neutral.
The University of Washington is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age, protected veteran or disabled status, or genetic information.
To request disability accommodation in the application process, contact the Disability Services Office at 206-543-6450 or dso@uw.edu.
COVID-19 VACCINATION REQUIREMENT
Governor Inslee's Proclamation 21-14.2 requires employees of higher education and healthcare institutions to be fully vaccinated against COVID-19 unless a medical or religious exemption is approved. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination. View the Final candidate guide to COVID-19 vaccination requirement webpage for information about the medical or religious exemption process for final candidates.",Educación,,2022-09-15T00:00:00,2022-10-16,OTHER
Jr. Data Engineer,Confluent,,,-1,-1,,USD,"Confluent is pioneering a fundamentally new category of data infrastructure focused on data in motion. Have you ever found a new favorite series on Netflix, picked up groceries curbside at Walmart, or paid for something using Square? That’s the power of data in motion in action—giving organizations instant access to the massive amounts of data that is constantly flowing throughout their business. At Confluent, we’re building the foundational platform for this new paradigm of data infrastructure. Our cloud-native offering is designed to be the intelligent connective tissue enabling real-time data, from multiple sources, to constantly stream across the organization. With Confluent, organizations can create a central nervous system to innovate and win in a digital-first world.
We’re looking for self-motivated team members who crave a challenge and feel energized to roll up their sleeves and help realize Confluent’s enormous potential. Chart your own path and take healthy risks as we solve big problems together. We value having diverse teams and want you to grow as we grow—whether you’re just starting out in your career or managing a large team, you’ll be amazed at the magnitude of your impact.
This role will be part of the Sales Strategy and Analytics group under Sales Operations. Specifically within the Sales Business Intelligence team, which is brand new and being built out this fiscal year. This team is the engine behind driving a self-serve reporting culture using Tableau for the broader Field Sales Operations organization.

About the Opportunity:
This newly created role will be a key member of the Sales Business Intelligence team and play an active role in accelerating Confluent’s Data Driven and Self-Serve Reporting culture. You will work as part of the team that transforms data into information, which serves as the foundation for analytics and insights. You will utilize your strong technical competencies to develop, establish and maintain datasets that feed our Tableau dashboards and other analytics. This position will report to the Manager of Sales Business Intelligence within the global Sales Strategy & Analytics organization This role will also be supporting our customer success and growth analytics (CSG Analytics) team.
What you will do:
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Google BigQuery and real-time systems such as Tableau.
Work with both the Sales Business Intelligence and CSG Analytics teams to develop, establish and maintain datasets to help drive dynamic tools and reporting.
Partner with analytics teams and business partners to produce internal data products (Certified Datasets)
Communicate changes and updates of owned data sources to key stakeholders
Building Data Pipelines - Create new pipelines or rewrite existing pipelines using SQL, Python on Airflow & DBT
Data Modeling - Partner with analytic consumers to improve existing datasets and build new ones
What you will bring:
1 to 3 years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, and BI tooling.
Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline.
Previous experience working with sales data and analytics.
Expert knowledge of SQL and of relational & cloud database systems and concepts.
Experience with BigQuery preferred; experience with any other relational data warehouse
Some knowledge of data architectures, data modeling, and data infrastructure ecosystem.
Experience with Enterprise Go-to-Market (GTM) business systems such as Salesforce, Marketo, Zendesk, Clari, Anaplan, etc.
Experience with project management tools such as Jira, Trello, and Confluence
Experience with ETL pipeline tools like Airflow, and DBT, and with code version control systems like Git.
The ability to communicate cross-functionally, derive requirements, and architect shared datasets.
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.
What gives you an edge:
Experience with Apache Kafka
Tableau
Knowledge of batch and streaming data architectures
Product mindset to understand business needs, and come up with scalable data engineering solutions
At Confluent, we are committed to providing competitive pay and benefits that are in line with industry standards. We analyze and carefully consider several factors when determining compensation, including your work history, education, and professional experience. This position has an annual estimated salary of $110,000 - , an annual bonus, and a competitive equity package. The actual pay may vary depending on your skills, qualifications, and experience.

In addition, Confluent offers a wide range of employee benefits. To learn more about our benefits click here.
#LI-Remote
#LI-DO2
Come As You Are
At Confluent, equality is a core tenet of our culture. We are committed to building an inclusive global team that represents a variety of backgrounds, perspectives, beliefs, and experiences. The more diverse we are, the richer our community and the broader our impact. Employment decisions are made on the basis of job-related criteria without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other classification protected by applicable law.
Confluent requires all employees (in office and remote) in the U.S. to be vaccinated for COVID-19. Consistent with federal, state, and local requirements, Confluent will consider requests for reasonable accommodation based on medical conditions/contraindications or sincerely-held religious beliefs where it is able to do so without undue hardship to the company.
Click here to review our California Candidate Privacy Notice, which describes how and when Confluent, Inc., and its group companies, collects, uses, and shares certain personal information of California job applicants and prospective employees.
#LI-Remote",Tecnologías de la información,,2022-09-08T00:00:00,2022-10-16,FULL_TIME
Junior Data Scientist/Database Engineer - Technology,Arena Investors LP,Florida,Jacksonville,-1,-1,,USD,"Please Note: This job is advertised for our office in Jacksonville Fl, but open to remote in the Southeast Region. (Florida, Georgia, North/South Carolina, Virginia, Alabama, Texas)
Arena Investors, LP (""Arena"") is a global investment management firm that seeks to generate attractive risk adjusted, consistent and uncorrelated returns by employing a fundamentals based, asset-oriented financing and investing strategy across the entire credit spectrum in areas where conventional sources of capital are scarce. Arena specializes in off-the-run, stressed, distressed, illiquid and esoteric special situation transactions through originations and acquisitions of asset-oriented investments across a wide array of asset types (including but not limited to private direct corporate credit, commercial real estate bridge lending, and commercial and consumer assets).
Quaestor Advisors, LLC (“Quaestor”) is an affiliated Special Servicer, which provides mid and back office services, including asset management, to Arena Investors and external clients.
Quaestor is looking to expand the Technology team, through the addition of a junior-to-mid-level database developer/data scientist who has a passion for coding, troubleshooting data and solving problems. The right candidate would be a key contributor to a proprietary financial data warehouse that is crucial to running its business. Responsivities would include analyzing data, troubleshooting logic problems, monitoring data health and developing data transformations/stored procedures while learning to interface with business-side sponsors from asset management, accounting, and operations. Ideal candidates will be organized, analytical, self-motivated, resourceful and able to work/communicate effectively with all internal functional groups.
This is a great opportunity to be a part of a large-scale fintech platform while honing one’s technical skills and learning the private credit business.
Responsibilities
Analyze/Troubleshoot financial data of Arena’s Data warehouse.
Assist with projects/tasks related to data analysis, data visualization, and reporting for our Operations, Finance & Asset Management departments.
Develop stored procedures and data transformations.
Develop PowerBI visualizations.
Requirements
Bachelor’s degree in computer science from a top university.
3-5 years experience in technology – tech pure play, startup, fintech, etc.
Internships/Co-ops experience is a plus.
Strong Relational Database knowledge (MS SQL Server, etc).
Fluency in SQL, T-SQL, Stored Procedures.
Familiarity with PowerBI (or Tableau) for data visualizations.
Familiarity with SSRS for reporting.
Experience with accounting, fund-accounting, trading operations-related data a plus.
Familiarity with web development (HTML5, Javascript, jQuery, CSS, REST, etc) a plus.
Familiarity with OOP and at least one language (C#, Java, etc) a plus.
Comfortable with Agile/Scrum SDLC.
A positive attitude, strong work ethic and a desire to work collaboratively across the organization.
Strong attention to detail.
COVID Vaccinated
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Family Leave (Maternity, Paternity)
Short Term & Long Term Disability
Training & Development
Work From Home
Free Food & Snacks
Wellness Resources",Finanzas,,2022-08-19T00:00:00,2022-10-16,FULL_TIME
Data Engineer - USA (Remote),Knoema,,,-1,-1,,USD,"What we do:
Knoema is the most comprehensive source of global decision-making data in the world. Our tools allow individuals and organizations to discover, visualize, model, and present their data and the world’s data to facilitate better decisions and better outcomes. They are used by more than half a million people worldwide every month.

What you'll be doing:
Build and maintain our data architecture to support reporting, analysis, dimensional modeling, and data development
Develop data management features such as master data, reference data, data quality, data catalog, and data publishing
Providing self-service data capabilities to help everyone leverage data and analytics
Helping to define and champion data quality practices and programs for Knoema data systems
Build and maintain data pipelines from internal and external databases
Create and maintain architecture and systems documentation
Write maintainable, performant code
Implement the DataOps philosophy in everything you do
Plan and execute system expansion as needed
Collaborate with Data Analysts to drive efficiencies for their work
Collaborate with other functions to ensure data needs are addressed



Technical Must Haves (Intermediate Level):
2+ years hands-on experience deploying production quality code
Professional experience using Python, Java, or Scala for data processing (Python preferred)
Knowledge of and experience with data-related Python packages
Deep understanding of relational databases, SQL, analytical data warehouses (Snowflake preferred), and query optimization techniques
Demonstrated ability to both diagnose and prevent performance problems
Hands-on experience implementing ETL (or ELT) best practices at scale.
Hands-on experience with data pipeline tools (dbt, AirByte, Airflow, Luigi)
Strong data modeling skills and familiarity with the Kimball methodology.
Constantly improve product quality, security, and performance
Desire to continually keep up with advancements in data engineering practices
Catch bugs and style issues in code reviews
Ship small features independently
Great applicants have:

The thirst to keep current on new industry trends, IT ops, and best practices, and can help guide us on what we should implement and why
A “no task is too small” attitude
Appreciation for the need to communicate the value of technical work to non-technical audiences: explaining ""why”, not just “what”
A recognition of the value of data to evaluate production environments with metrics, monitoring, and dashboards
Knoema Non-Negotiables:

Willing to work on a team that believes:
Everything starts with the customer
Deliver first, optimize soon
Small experiments >>> large releases
Debates are a sign of passion, and are never personal
Ownership is key
Viewing diversity as a core strength, not just a nice feature
Embracing a growth mindset to tackle new, interdisciplinary challenge

We have multiple Data Engineer roles open, of varying experience levels:
Junior:

At Knoema this is the entry level of roles in Engineering as their first job in the tech industry, new graduate, or returning to the workforce.

Intermediate:

Intermediate Data Engineers are expected to meet the requirements and execute the responsibilities above with minimal assistance.
Senior:

Understand and implement data engineering best practices
Improve, manage, and teach standards for code maintainability and performance in code submitted and reviewed
Create smaller merge requests and issues by collaborating with stakeholders to reduce scope and focus on iteration
Ship medium to large features independently
Generate architecture recommendations and the ability to implement them
Great communication: Regularly achieve consensus amongst teams
Perform technical interviews

Staff:

Ship large features and foundational improvements with minimal guidance and support from other team members
Support other data team members with the shipping of new features by setting direction and providing guidance
Solve technical problems of the highest scope and complexity
Exert significant influence on the long-range goals
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Represent Knoema and its values in public communication around broad initiatives, specific projects, and community contributions
Interact with customers and other external stakeholders as a consultant and spokesperson for the work of your team
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of our Data Architecture",Tecnologías de la información,,2022-08-24T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,New York Public Radio,New York State,New York,-1,-1,,USD,"Junior Data EngineerThe Data team is seeking a Junior Data Engineer to work on various tasks and projects. The members of Data the team uses rigorous analytics to generate insights that inform content, product, marketing, and business decisions across New York Public Radio. In the meantime, we build systems, infrastructure, data products for collecting, storing, and visualizing data to foster data democracy at New York Public Radio. We work in Python, SQL and we work with technologies like AWS Glue, Lambda, S3 and PySpark.The main functions of this role include:1. ETLDesign and build ETL pipelines by using AWS Glue to collect data from different sources to data warehousesBuild pipeline integration with our various data products for long-running processes. This will include CRM data ( Salesforce ), payments processors ( Stripe & Springboard ), email ( Salesforce Marketing Cloud), our own digital platforms ( apps and websites Google Analytics data ) and audio CMS data, which comes in log form.Identify the room for optimizing relational data storage through design, query optimization, indices, replicas, partitioning, etc.2. Automation: *Build scripts or recurring processes to fully/partially automate labor-intensive workflows of other departments (e.g. Content, Marketing, Fundraising, the Newsroom) as well as for running models and analysis on behalf of the Data team.Requirements & Qualifications1-2 years hands-on experience in ETL design, implementation and maintenanceExperience in schema design and data modeling and in writing complex SQL queries to extract data from relational databases (e.g. MySQL, Redshift)Experience in the following tools/technology is desired: Python, PySpark, AWS Glue, Lambda, S3, Redshift and Google Cloud PlatformComfortable with extensive Python coding. pySpark is a plus.Willing to learn any language/tools/frameworks necessaryExcellent skills in communicating technical knowledge to a broad non-technical audienceExcellent organizational skills with the ability to manage several projects concurrently and communicate effectively across multiple divisions and with diverse groups of stakeholdersAdditional InformationThis is a full-time role with a salary range of $80-$85K per year (plus a full benefits package). Salary offer within this range is determined by skills, experience and organizational pay equity. New York Public Radio offers excellent medical, dental, and vision insurance, vacation, personal and sick time as well as parental leave.This role is currently operating in a primarily remote capacity but the selected candidate will have the option to work from the office if fully vaccinated against COVID-19, unless NYPR has approved a specific religious or medical exemption in advance of the first day of work. At this time, NYPR defines being fully vaccinated as having received two Moderna or Pfizer COVID-19 vaccinations or one Johnson and Johnson vaccination.This role, if desired by the selected candidate, could remain remote.Commitment to Diversity, Equity & InclusionNew York Public Radio is committed to diversity, equity, and inclusion. We continuously strive to place our employees at the center of our thinking and elevate inclusive practices to develop and support a more engaged and productive workforce. Our journalism and operations in the service of that journalism benefit from a broad range of perspectives, from all backgrounds, at all levels of the organization. Diversity is essential to honest, authentic, accurate storytelling and reportage; creating an institution in which all voices are encouraged, valued, and heard.Equal OpportunityNew York Public Radio is an equal opportunity employer committed to achieving the goal of equal employment opportunity for all. Applicants and employees are considered and evaluated for positions without regard to mental or physical disability, race, creed, color, religion, gender, national origin, citizenship status, age, genetic information, military or veteran status, sexual orientation, marital status, employment status or any protected federal, state or local status unrelated to the performance of the work involved.#LI-remoteJob Type: Full-timePay: $80,000.00 - $85,000.00 per year",Audiovisual y medios de comunicación,,2022-09-15T00:00:00,2022-10-16,FULL_TIME
Junior Engineer- Data Entry,Zodiac Solutions,Tennessee,Collierville,-1,-1,,USD,"Job Title: Junior Engineer- Data EntryJob Location: Collierville, Tennessee (Second Shift )Duration: Fulltime Job Description: Perform data entry activities using MS office(Word, excel must)Diploma/ Bachelors of Science in Mechanical/industrial engineeringJob Type: Full-timeSchedule:8 hour shiftMonday to FridayAbility to commute/relocate:Collierville, TN: Reliably commute or willing to relocate with an employer-provided relocation package (Required)Experience:Microsoft Excel: 1 year (Preferred)Work Location: One location",Tecnologías de la información,,2022-08-26T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer - HYBRID,Kearny Bank,New Jersey,Fairfield,-1,-1,,USD,"Overview: You will be hands on assisting in the creation of data assets, pipelines, reports, and queries that will empower Business Users to perform their functions at scale. Data and information have become an invaluable asset for Banks that focus on innovation and a great user experience. Kearny Bank takes this concept to heart and the Data Analytics team is at the center of this initiativeResponsibilities:Assist in the development of data assets, reports, queries, and models that enable Bankers and other stakeholders across the organizationContribute to documentation around critical production data processesAssist in managing Kearny Bank’s reporting systems and data request management systemsRecognize and adopt best practices in developing data reports and analytical insights including data integrity, testing, analysis, validation, and documentation.Assist in design, development, and maintenance of scalable and automated next generation data solutionsQualifications:Working knowledge of database systems, Excel, and Business Intelligence toolsUnderstanding of Data Modeling concepts and toolsExperience in collaborating with individuals and organizationsCRM experience preferredExperience with Microsoft Azure preferredExperience with Microsoft 365 preferredGeneral Banking and Financial experience preferredJob Type: Full-time",Finanzas,,2022-09-06T00:00:00,2022-10-16,FULL_TIME
Junior/Mid-level Software Engineer - Data Engineering,The Trade Desk,California,Irvine,-1,-1,,USD,"The Trade Desk is a global technology company with a mission to create a better, more open internet for everyone through principled, intelligent advertising. Handling over 600 billion queries per day (more than 100 times the query volume of search globally), our platform operates at an unprecedented scale. We have also built something even stronger and more valuable: an award-winning culture based on trust, ownership, empathy, and collaboration. We value the unique experiences and perspectives that each person brings to The Trade Desk, and we are committed to fostering inclusive spaces where everyone can bring their authentic selves to work every day.
Do you have a passion for solving hard problems at scale? Are you eager to join a dynamic, globally-connected team where your contributions will make a meaningful difference in building a better media ecosystem? Come and see why Fortune magazine consistently ranks The Trade Desk among the best small-medium-sized workplaces globally.
About the Role:
Our Data Engineering Software Engineers are end-to-end owners. You will participate actively in all aspects of designing, building, and delivering data products for our clients.
You will work with petabyte-scale data challenges, large-scale distributed systems coordinating thousands of servers in cloud and physical data centers around the world, machine learning, and advanced visualizations – to name a few.
You will work with data processing pipelines, ML pipelines, data processing automation, data governance, data visualization, data quality, data privacy, data warehousing.
Our Software Engineers work with a variety of platforms and technologies, such as Docker, Kubernetes, Gitlab, Bamboo, AWS, Azure, Scala, Spark, SQL Server, and Vertica.
Who We Are Looking For:
You understand engineering and computer science fundamentals. At our scale, many off-the-shelf techniques and existing technologies (open source and enterprise) simply don't work. You are able to work from first principles to evaluate solutions and adapt them to a unique environment.
You are passionate about data engineering. You'll work at petabyte-scale with SQL, ETL, data modeling, and technologies similar to Spark, Scala, C#, Java, etc..
You are a creative thinker, not bound by ""the way things have always been done"". What you know is less important than how well you learn and innovate. We don't need engineers who know all the answers; we need engineers who can invent the answers no one has thought of yet, to the questions yet to be asked.
What We Care About:
What and how you can contribute is what’s important to us. Our consideration is not limited by the kind of education you have or the specific technologies you have experience with. Variety of technical challenge is one of the best things about working at The Trade Desk as an engineer, but we do not expect you to know every technology we use when you start. What we care about is that you can learn quickly and solve complex problems using the best tools for the job.
Our culture runs much deeper than just having fun together (though, we do that well too...) – the people we want on our team are trust-builders, generous givers, scrappy problem solvers, and gritty pursuers of excellence.
Does this sound like you? If so, we welcome your application and the chance to meet you.
#LI-RE1
Our Compensation and Benefits (for Colorado residents only)Base Compensation Range: $95,800 - $191,600
In accordance with Colorado law, the range provided is The Trade Desk’s reasonable estimate of the base compensation for this role. The actual amount may be higher or lower, based on non-discriminatory factors such as experience, knowledge, skills and abilities. The Trade Desk also offers a competitive benefits package. Click here to learn more.
The Trade Desk does not accept unsolicited resumes from search firm recruiters. Fees will not be paid in the event a candidate submitted by a recruiter without an agreement in place is hired; such resumes will be deemed the sole property of The Trade Desk. The Trade Desk is an equal opportunity employer. All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.",Tecnologías de la información,,2022-08-26T00:00:00,2022-10-16,FULL_TIME
Jr. Data Analyst/Engineer,Gate3,Illinois,Elk Grove Village,-1,-1,,USD,"Summary: Leads small projects or supports larger initiatives as part of an Agile team and partners with users to understand business requirements. Develops data objects for business analytics using data modeling techniques. Ability to develop and maintains moderately complex automated ETL pipeline architecture using assigned tools and programming languages. Analyzes, develops, and maintains data warehouse and landing zone metadata, data catalog, and user documentation for internal business customers. Ideal candidate would have SQL, data warehousing knowledge, Meltano, and DBT experience.Essential Functions: Develops, tests, and maintains pipeline architecture and infrastructure: Develops and maintains moderately complex automated ETL pipeline architecture using assigned tools and programming languagesDevelops and maintains moderately complex automated ETL monitoring and alarming solutions using assigned languages and servicesImplements and supports reporting and analytics infrastructure for internal business customers using AWS servicesDevelops, tests, and deploys code using internal software development toolsets, including the code for deploying infrastructure and solutions for secure data storage, ETL pipelines, data catalogs and data queriesMaintains legacy data solutions to ensure functionality until replaced with new technologyData analysis: Leads small projects or supports larger initiatives as part of a teamPartners with users to understand business requirementsCollaborates with team on conceptualizing and developing new data solutions to meet the business requirementsResearches, performs analysis and proposes effective solutions related to systems development and enhancementsReviews potential adjustments or modifications for impacts on other programsCollaborates with business areas to develop solutions in the legacy systems to meet business requirementsData modeling: Develops data objects for business analytics using data modeling techniquesDevelops and optimizes data warehouse and landing zone tables using best practices for data definition language (DDL), physical and logical tables, data partitioning, compression and parallelizationEducation & Experience: Bachelor’s degree, preferably in a computer related field or equivalent relevant experienceTwo years of data integration development experience, or related experienceKnowledge, Skills & Abilities: Good SQL knowledge and experience working with relational databasesGood analytic skills to work with unstructured datasetsGood knowledge of applicable programming languages, such as PythonWorking knowledge of AWS cloud services such as S3, Glue, Athena, Cloudwatch and Lambda preferredKnowledge of Snowflake or Redshift preferredKnowledge of DBTpreferredStrong ability to build processes supporting data transformation, data structures, metadata, dependency and workload managementGood ability to manipulate, process and extract value from large disconnected datasetsStrong verbal and written communication skillsStrong attention to detail, organizational and multi-tasking skills required with the ability to adapt to changing prioritiesAbility to maintain confidentialityJob Types: Full-time, Part-time, ContractPay: $60,000.00 - $80,000.00 per yearBenefits:Dental insuranceEmployee assistance programFlexible scheduleHealth insuranceLife insurancePaid time offParental leaveProfessional development assistanceReferral programRetirement planTuition reimbursementVision insuranceSchedule:8 hour shiftCOVID-19 considerations:Our office building follows local guidelines in common areas and our company follows local and federal vaccine mandates.Ability to commute/relocate:Elk Grove Village, IL 60007: Reliably commute or planning to relocate before starting work (Required)Experience:Advanced SQL: 3 years (Required)Data warehouse: 1 year (Required)AWS or Azure: 1 year (Preferred)Python: 2 years (Required)Data visualization: 1 year (Required)R: 1 year (Preferred)Work Location: One location",,,2022-08-31T00:00:00,2022-10-16,FULL_TIME
"Data Engineer Trainee, Junior",Booz Allen Hamilton,Virginia,Norfolk,-1,-1,,USD,"The Challenge:
Are you excited at the prospect of unlocking the secrets held by a data set? Are you fascinated by the possibilities presented by cloud engineering and loosely coupled architectures? In an increasingly connected world, massive amounts of structured and unstructured data open new opportunities. As an aspiring cloud data scientist, you know you can help turn these complex data sets into useful information to solve global challenges. Across private and public sectors — from fraud detection, to cancer research, to national intelligence — you see data scientists turning data into actions and you want to be part of the team.
We have an opportunity for you to develop your analytical skills and establish your career in data science. You’ll join a rigorous training program that combines skills assessments, a comprehensive curriculum, functional mentorship, a capstone analytic challenge, and support to place you on your first data science project. You’ll learn how to write scripts to integrate data, conduct exploratory data analysis to discover hidden trends, apply machine learning in AWS to train predictive models, and use the right combination of tools and frameworks to turn that set of disparate data points into objective answers that inform decisions. Equipped with the foundational data science skills to accelerate your career, you’ll join a team and apply those skills to support our clients’ critical national security missions. Embrace the challenge and join us in driving change through data science. This position is a hybrid role with a combination of working at a Booz Allen office or client site and working remotely.
Empower change with us.
You Have:Experience with querying or analyzing data to answer questions and solve problemsExperience with visualizing data to identify or communicate key insightsKnowledge of basic concepts in mathematics and statisticsKnowledge of Cloud environmentsAbility to obtain a security clearanceBachelor’s degree
Nice If You Have:Experience with systems engineering or systems administrationExperience with Amazon Web ServicesExperience with AzureExperience with Docker, Kubernetes, and AnsibleKnowledge of Python and GitHubAbility to learn a programming languageSecret clearanceBachelor’s degree in Data Science, Mathematics, Engineering, Physics, Statistics, or CS
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Build Your Career:
At Booz Allen, we know the power of analytics and we’re dedicated to helping you grow as a data analysis professional. When you join Booz Allen, you can expect:
access to online and onsite training in data analysis and presentation methodologies, and tools like Hortonworks, Docker, Tableau, and Splunk
a chance to change the world with the Data Science Bowl—the world’s premier data science for social good competition
participation in partnerships with data science leaders, like our partnership with NVIDIA to deliver Deep Learning Institute (DLI) training to the federal government
You’ll have access to a wealth of training resources through our Analytics University, an online learning portal specifically geared towards data science and analytics skills, where you can access more than 5000 functional and technical courses, certifications, and books. Build your technical skills through hands-on training on the latest tools and state-of-the-art tech from our in-house experts. Pursuing certifications? Take advantage of our tuition assistance, on-site boot camps, certification training, academic programs, vendor relationships, and a network of professionals who can give you helpful tips. We’ll help you develop the career you want, as you chart your own course for success.
We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",Administración y consultoría,,2022-09-02T00:00:00,2022-10-16,OTHER
Junior Data Engineer,Decision Point Healthcare,Massachusetts,Boston,-1,-1,,USD,"Boston, MA
Full-Time
We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Experience with Pandas/Numpy in a production environment
Experience with CI/CD and version control tools: Git preferred
Experience working within hybrid cloud environment; AWS experience is a plus
Excellent verbal and written communication
Excellent listener and collaborator with senior leaders, peers, and staff
Familiarity with data engineering and workflow management frameworks such as Airflow and dbt
Familiarity with healthcare data is a plus
A little bit about Decision Point:
We are a rapidly growing healthcare technology company changing the fundamentals of patient and provider engagement. For years, health plans have relied on descriptive data and reactive engagement. We empower our clients to understand and predict the whole member journey, enabling sustained improvements in member health outcomes and plan performance. We combine the latest, most practical technologies and a deep understanding of healthcare, bringing innovative, pragmatic solutions to an industry that touches us all.",,,2022-08-17T00:00:00,2022-10-16,FULL_TIME
Jr. Systems Engineer (Data Flow & Routing Solutions),Themis Insight,Maryland,Fort Meade,-1,-1,,USD,"Themis Insight solves difficult business, IT, and analytic problems by addressing the whole problem – not just the symptoms – using interdisciplinary approaches that are both practical and innovative. We provide fresh alternatives to ordinary, mainstream consulting firms through small, highly skilled, and hand-picked teams that can meet clients' needs in any industry. Our broad interdisciplinary understanding allows us to provide the right solution, even if it is from outside the industry or traditionally defined problem space. We bring Public and Private, Civilian and Military expertise to every case.


We are hiring a Jr. Systems Engineer (Data Flow & Routing Solutions) to work in Fort Meade, MD. Position location is subject to change based on central MD client's needs.
Required: TS/SCI with a polygraph
Provide support to the Data Flow and Routing Solutions (DFRS) systems engineering team. Required support includes customer service, dataflow paths, organize dataflow implementation, identify ways to improve dataflow implementation and modernization efforts, and prioritize the customer's needs based on larger strategic priorities.
Responsibilities include:
Identify and establish new processes and procedures (modernization efforts, process improvement…) and have good customer service skills.
Data tagging, data formats, data sanitization, data sharing rules/criteria, general dataflow knowledge and troubleshooting.
Provide support to manage the progress of dataflow requests. This includes customer and service provider support/interaction, and learning the customer need and how it fits into the larger picture.
Measure and monitor metrics, identify trends, document deficient resourcing, and report standings and findings to management and other stakeholders.
Provide support to technical leaders by coordinating activities pertaining to the development, documentation and maintenance associated with changes to the system.
Research and document dataflow processes according to standards of compliance in support of the Enterprise Data Header (EDH) initiatives. Support will include research, analysis, documentation, and integration of how data can be identified, protected, tracked and handled throughout its life cycle.
Individual Capabilities/Experience Required:
A Bachelor's degree in Computer Science, Electrical Engineering, Systems Engineering, or a related discipline. Note: a High School Diploma or GED plus 5 years of systems engineering experience would also be acceptable.
Excellent oral and written communications skills.
This is a full time position requiring 1880 hours of support per year.
Position requires TS/SCI clearance with polygraph
Individual Capabilities/Experience Desired:
Experience in the Data tagging, data formats, data sanitization, data sharing rules/criteria, general dataflow knowledge
Ability to work constructively and successfully with diverse stakeholders to resolve mission and technical issues is critical
A self-starter, have a high level of attention to detail, and possess excellent oral and written communication skills
Themis Insight has all the PERKS!
You are our most valuable resource — your ambition, your knowledge, your creativity. We offer an industry-leading set of benefits to supplement your normal salary compensation. Themis Insight has you covered with flexible ways to balance work and home life, full health benefit premium coverage, and generous contributions toward your retirement.
Competitive health, dental, and vision plans with 100% paid premiums.
401k: We contribute 6% even if you don't!
Time Off: 11 standard holidays, and 20 days of PTO
Career Development: Get career counseling and individualized career development plans, including education and training.
Employee referral bonuses for successful hires
Themis Insight is an Equal Opportunity/Affirmative Action employer.",,,2022-09-03T00:00:00,2022-10-16,FULL_TIME
Junior CAT Modeling Analyst/Data Engineer,Berkley,New Jersey,Morristown,-1,-1,,USD,"Company Details:
Berkley One exists to insure and protect the lifestyles of a modern generation of affluence. We seek clients that are sophisticated individuals and families who require world-class risk and claims management customized to their needs, a team of select expert independent agents and innovative digital tools to keep it simple and easy. We’ll blend our partners, products and capabilities with all that is Berkley, generating a modern solution for the customers we serve.
Our culture is one of innovation, creativity and teamwork. Our team is highly motivated, passionate about our business, and deeply experienced in developing and delivering product and service solutions in the personal insurance marketplace.
Responsibilities:

We have an exciting opportunity for a Junior CAT Modeling Analyst/Data Engineer to join our team! Reporting to our Chief Analytics Officer, you will support multiple aspects of our business including, analytics, business intelligence, and enterprise reporting. You will bring strong analytical abilities and an inquisitive mind that can translate the stories found in the data.
We have a welcoming culture valuing our employees – we actually trademarked the phrase Everything Counts, Everyone Matters® to describe the Berkley commitment to our people and how we do business. We believe that every person in the organization is important and every accomplishment makes a difference in our results. Come join us!
This position can be located in: Morristown, NJ; Boston, MA; or New York, NY
What you'll do:
Responsible for the day-to-day hands on development and maintenance of our data warehouse environment, integrated business intelligence & visualization tools, and batch data extraction processes
Support and educate user population on the use of BI and visualization tools
Partner with leadership, engineers, program managers and analytics to understand data needs then design and develop appropriate data solutions
Partner with core system vendor and application development team to ensure a clear and consistent data model is maintained along with an associated data dictionary
Perform data and system analysis for defects and incidents, articulate root causes, and implement solutions
Support areas include inconsistencies/inaccuracies in the data warehouse, batch data extracts for 3rd party integrations, financial & statistical reporting, and enterprise data feeds
Qualifications:

What we're looking for:
BS or MS degree in Computer Science/Engineering or a related quantitative field
2+ years SQL knowledge and experience working with relational databases, query authoring, and a working familiarity with a variety of databases
2+ years experience with Data Modeling – designing and building models for relational and semi structured data
Strong ETL/ELT knowledge
Experience with data visualization tools such as Power BI or Looker
Programming experience (Python/R) preferred
Experience with data pipelines preferred
Experience with Azure Cloud Data Solution preferred
Exceptional oral and written communication skills, a communication style flexible to the situation, and able to communicate clearly and with a purpose
Calm under pressure, possessing excellent organizational skills, integrity, and follow-through on tasks, able to challenge the norms while working collaboratively with colleagues at all levels of the organization
Natural curiosity, a love of learning how things work and always on the lookout for innovative improvements
Additional Company Details: The Company is an equal employment opportunity employer Additional Requirements: COVID-19 vaccine required unless prohibited by law",Manufactura,,2022-09-14T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,Illumination Works LLC,,,-1,-1,,USD,"Location and Travel Details: Remote
 The key responsibilities of the Data Engineer include:
Work closely with data scientists to ingest, integrate, and prepare database structures needed to advanced analytics and visualization
Establish databases for structured and unstructured data as well as improve code performance and efficiency for production applications
Heavily involved in the ETL process and must be motivated to be collaborative with the data science team to understand and then implement data requirements
Do you have what it takes? Are you driven to implement creative solutions that unravel complex and ever-changing challenges? We value passion, curiosity, and perseverance with an ability to communicate ideas and results to diverse audiences. We look for people who thrive in collaborative and independent assignments, have the aptitude to learn new data quickly, and who are willing to mentor junior team members. Key skills for this position include:
Database design and principles
Data modeling, schema development, and data-centric documentation
Experience integrating data from a variety of data source types
Recommend and advise on optimal data models for data ingestion, integration and visualization
Develop and maintain ETL pipelines
Strong SQL skills
Proficiency with Python and Spark SQL
Experience improving code performance and query optimization to meet demanding data analytic and visualization needs
Experience with multithreaded or multiprocessing code
Understanding of cloud (AWS/Azure), big data, and development best practices
Outstanding problem-solving skills
Excellent verbal and written communication skills and the ability to interact professionally with a diverse group, executives, managers, and subject matter experts
 Additional Desired Qualifications
Experience with both structured and unstructured data
Experience with big data and data analytics tools/frameworks (Hadoop, Hive, Spark, R, TensorFlow, PyTorch, Couchbase)
Experience with bash scripting
Experience with C/C++
Collaborative coding experience and competency with Git
Minimum Education: Bachelor’s or Master’s degree in Computer Science, Mathematics, or comparable academic discipline
Minimum Experience Requirements: Three (3) years of experience required
Must have or be willing to obtain Secret Clearance (this requires US Citizenship)
Acceptable candidates must successfully pass a drug test and background screen
A little more about us. At Illumination Works, we know data, and we should, we’ve been doing it since we started in 2006! We specialize in everything data from big data to data science, software engineering, data management, AR/IoT, and cloud development. Illumination Works is a trusted technology partner in user-centered digital transformation—delivering impactful business results to clients. We partner with customers to solve their unique technology and data challenges, and stay on top of modern technologies and advancements leveraging our Innovation Lab.
Illumination Works is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity Employer, making decisions without regard to race, color, religion, sexual orientation, gender identity or national origin, age, veteran status, disability, or any other protected class. Acceptable candidates must successfully pass a drug test and background screen.
Why choose us? Taking care of our people is our number one priority. We offer market competitive salary, a generous PTO package, and comprehensive medical, dental, vision and life insurance plans. We also offer 401K, short/long-term disability insurance, a fun and engaging culture, and training opportunities to keep you up to speed on the latest technologies.
Illumination Works is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity Employer, making decisions without regard to race, color, religion, sexual orientation, gender identity or national origin, age, veteran status, disability, or any other protected class. Acceptable candidates must successfully pass a drug test and background screen.",Tecnologías de la información,,2022-09-16T00:00:00,2022-10-16,OTHER
Junior Voice/Data Communications Engineer,Zenetex LLC,Maryland,Patuxent River,-1,-1,,USD,"Position Overview:

ZENETEX specializes in management and technology support services for a variety of federal agencies and commercial organizations. We don’t just say “Because Service Matters”, we prove it in working side by side with our customers across a wide range of federal agencies. Each of these agencies is involved in the defense of our country, which matters to all of us as citizens. The service we provide allows them to achieve their very valuable missions.

At ZENETEX, we empower our employees to make experience-based decisions for our customers; their hard work and commitment to the mission are both recognized and rewarded. We have an opportunity for an experienced Voice/Data Communications Engineer to join the Video Technologies Team at Patuxent River Naval Air Station.
Description:
Applies knowledge of telecommunications systems and audio video production techniques, sound slide presentations, and peripheral support equipment to provide robust and fully functional Video Teleconferencing (VTC) capabilities.
Leads install team in modifying, installing, testing, evaluating, and operating electrical, electronic, video, audio, computer network or related communication equipment. Maintains repairs, inspects, and troubleshoots or programs systems, equipment, and components. Reviews, analyzes, or applies technical or maintenance specifications, policies, and standards
Interfaces and coordinates with customers regarding system requirements determination, specification alternatives and project solutions
Qualifications:

Desired Experience:
Resolution of VTC issues in a high volume, fast paced environment
Installing cables, wires and telecommunications equipment
Providing VTC support to multiple work sites
Daily use of a ticketing system to manage workload and end-user assistance
Troubleshooting of basic IT services, desktop systems, and peripheral devices
Desired Skills:
Excellent customer service, communication and organizational skills
Able to climb ladder and lift, carry, move, load and unload heavy objects (up to 50 pounds)
Will be using power tools and hand tools
Ability to read and interpret specifications and drawings
Wire-wrapping, lacing rack mounting and cable running
Computer skills a plus
Requests for reasonable accommodations will be considered to enable individuals with disabilities to perform the principal (essential) functions of this job.
Security Clearance:
This position will require U.S. citizenship and an active DoD Secret clearance.
ZENETEX is an equal opportunity/affirmative action employer. Qualified applicants are considered for employment without regard to age, race, national origin, sexual orientation, gender, gender identity and expression, disability, veteran status, or any other characteristic protected by law.","Servicios de construcción, reparación y mantenimiento",,2022-08-28T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,Serenity Healthcare,Utah,Lehi,-1,-1,,USD,"Junior Data Engineer
Serenity Healthcare is hiring a Junior Data Engineer for our Lehi, UT headquarters. (Remote availability for residents of Utah or Colorado) While previous ETL experience is preferred, we are open to exceptional entry-level talent for this role.
We intend to provide on the job training in data-skills: SQL, BI (PowerBI), ETL (SSIS), Warehousing (SQL Stored Procedures), Exploratory Data Analysis, etc. It’s our intention to train you in Microsoft’s new tool: PowerApps.
Desired skill sets:
Must be a quick learner
SSIS experience strongly preferred
Skills used in the role:
SQL 20%
SSIS 40%
PowerApps 40%
Day-to-day work description:
The Junior Data Engineer will be responsible for keeping the data flowing, building new data pipelines, and creating business applications using MS-PowerApps. You’ll need to be comfortable with SQL, SQL Server, and SSIS. You’ll be reading API documentation to establish new ETL flows, as well as automating report delivery.
Job Fit:
Capable of “Deep Work”
Problem Solver
Reliable and consistent
Attention to detail
What We Offer to You:
Competitive pay (DOE), including additional target compensation
Opportunity to work and grow your career in a fast-paced environment
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment

Who We Are:
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
 Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.",Salud,,2022-09-01T00:00:00,2022-10-16,OTHER
Jr. Data Engineer,PepsiCo,Texas,Plano,-1,-1,,USD,"Responsibilities

PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.

What PepsiCo Data Management and Operations does:
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
Job Description

As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.

Accountabilities
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Understand and adapt existing frameworks for data engineering pipelines in the organization.
Responsible for adopting best practices around systems integration, security, performance, and data management defined within the organization.
Collaborate with the team and learn to build scalable data pipelines.
Support data engineering pipelines and quickly respond to failures.
Collaborate with the team to develop new approaches and build solutions at scale.
Create documentation for learning and knowledge transfer.
Learn and adapt automation skills/techniques in day-to-day activities.

Qualifications

1+ years of overall technology experience includes at least 1+ years of hands-on software development and data engineering.
1+ years of development experience in programming languages like Python, PySpark, Scala, etc.Experience or knowledge in Data Modeling, SQL optimization, performance tuning is a plus.
6+ months of cloud data engineering experience in Azure. Azure Certification is a plus.
Experience with version control systems like Github and deployment & CI tools.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools is a plus.
Experience in working with large data sets and scaling applications like Kubernetes is a plus.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
Education
BA/BS in Computer Science, Math, Physics, or other technical fields.
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, and the ability to influence and demonstrate confidence in communications with senior-level management.
Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to coordinate effectively with the team.
Positive and flexible attitude and adjust to different needs in an ever-changing environment.
Foster a team culture of accountability, communication, and self-management.
Proactively drive impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to learn quickly and adapt to new skills.
Competencies
Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
Understands both the engineering and business side of the Data Products released.
Places the user in the center of decision making.
Teams up and collaborates for speed, agility, and innovation.
Experience with and embraces agile methodologies.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.
Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter.",Manufactura,,2022-09-14T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,"Management Controls, Inc.",,,-1,-1,,USD,"Description: Management Controls is looking for a high-impact and versatile Junior Data Engineer to join our team. This role will assist the technical core of our data analytics service offering. The candidate must be collaborative by nature—willing to lead, execute, optimize, problem solve and adjust both work product and processes—based on feedback and input from multiple stakeholders.As a Junior Data Engineer, you will assist in building scalable data and analytics models and architecture from event formation to ingestion to reporting. You will evaluate the implementation of robust systems used by not only the Management Controls team but our customers and their vendors globally. You will help shape the vision of the end-to-end data pipeline architecture while following industry best practices.The business is privately held, has been around for 30 years, services most of the Fortune 100, and is experiencing exponential growth, which will last into the next decade. In addition to the normal tech company benefits like stocked kitchens, an in-house gym, bonuses, and flexible PTO. The company also offers 100% paid healthcare. So, if you want to work in place that takes care of you and those you love, values creativity, learning, and individual contribution think about joining the team.This position is open to candidates located outside of the Houston, Texas area that reside within the continental US.Duties and responsibilitiesIdentifying, designing, and implementing internal process improvementsAutomate manual processes, optimize data delivery, re-design infrastructure for greater scalability, etc.Analyse and organize raw dataInterpret trends and patternsPrepare data for prescriptive and predictive modellingMaintain the extraction, transfer, and load (ETL) of data through PythonImplement new/existing client databases in our ETL, as necessaryDevelopment and use of analytical/data tools to assist in analyticsCreating and maintaining optimal data pipeline architectureCreating visualizations based on prepared dataWork with stakeholders (Insights, Infrastructure, Product, etc.) to assist with any data-related technical issues and support their data infrastructure needsRequirements: Required:Bachelor's degree or higher in a quantitative/technical field (i.e. Computer Science, Statistics, Engineering) or equivalent experienceHands on knowledge with a scripting language (preferably Python)Hands on knowledge with SQL to write complex, optimized queriesUnderstanding of data warehousing and modelling conceptsExperience with business intelligence applications i.e. Power BI, Tableau, Logi, etc. with a preference for proficiency in Power BIWork with stakeholders (internal/external) to assist with any data-related technical issues and support their data infrastructure needsStrong problem solving and troubleshooting skillsA positive attitude, open mind, and a desire to collaborate with and learn from multi-disciplinary teamsInterest in and ability to adapt and learn emerging technologiesPreferred:Experience in AWS or Azure cloud servicesDemonstrated experience in working with APIs, data modeling, ETL, and data warehousingExperience with messaging/alerting infrastructure (Twilio/SendGrid)California applicants: please see our privacy notice here.Job Type: Full-time",,,2022-08-24T00:00:00,2022-10-16,FULL_TIME
Jr Data Application Engineer,Tyler Technologies,,,-1,-1,,USD,"Tyler Technologies is currently seeking a Jr. Data Application Engineer to join their US Direct / NIC Division to support their Recreation Dynamics product. This is a remote position. As our Jr. Data Application Engineer, you will focus on supporting our government partners and ensuring support and satisfaction to our Outdoor products.
Summary
The ideal candidate will be able to manage communications with our clientele as the primary point of contact for complex analytical, technical and administrative support in a SQL Server environment. You will be expected to quickly gain the knowledge of our software system and customer business areas in order to be effective at this role. The capability to handle multiple inquiries / projects with varying deadlines and in a fast-paced office environment is a must.
Key Responsibilities
Will provide complex analytical, technical, and administrative support to facilitate the day to-day operations.
Respond to complex inquiries from administrators.
Perform analytical and technical tasks to complete special and ongoing projects requiring extensive research, data collection, and detailed analysis, following departmental guidelines, policies, and procedures.
Exercise independent judgment to troubleshoot and resolve issues.
Experience / Education
Degree in Computer Science is preferred
SQL experience would be a plus
0-3 years of experience in a similar role
Solid analytical and problem-solving skills involving sound decision making and effective resolutions
Keen attention to detail.
Strong planning and organizational skills involving the ability to manage multiple work streams effectively.
Ability to work within a team to meet established project goals
Ability to communicate professionally, concisely and effectively, both verbally and in writing, to internal and external stakeholders",Tecnologías de la información,,2022-08-23T00:00:00,2022-10-16,OTHER
Junior Data Engineer - Secret Clearance,Deloitte,Virginia,Rosslyn,-1,-1,,USD,"In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.


Work you'll do


We are looking for experienced Data Engineers to build and deliver innovative, game-changing mission-driven data pipelines. On this project, you will be responsible for leading the architecture and setup of hosted data lakes, as well as the ingestion pipeline and processing for large datasets, working closely with Agile software development team(s). This role includes responsibilities such as creating and managing schedules for data management (migration, integration, etc.) efforts, working with clients to validate migrated data, working with Agile development teams to understand changes and their impacts towards data migration efforts, among other tasks.

The team


Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.


The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.


Qualifications

Active secret security clearance required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
1+ years of experience with extract, transform, and load (ETL) methods and tools
1+ years of experience with data modeling, data warehousing, and building ETL pipelines
1+ years of experience with SQL queries and JSON objects
1+ years of experience with both SQL and NoSQL databases, including PostgreSQL and MongoDB
Familiarity with microservice architectures
Interest in event streaming architectures, such as Apache Kafka
Prior professional services or federal consulting experience
Knowledge of data mining, machine learning, data visualization and statistical modeling
Ability to thrive in a fast-paced work environment with multiple stakeholders
Creativity and innovation - desire to learn and apply new technologies, products, and libraries

Ability to travel up to 10%, on average, based on the work you do and the clients and industries/sectors you serve.


How you'll grow


At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.

#LI-DP4",Finanzas,,2022-08-24T00:00:00,2022-10-16,FULL_TIME
"Junior Software and Data Engineer, Systematic Equities",Millennium Management LLC,California,Newport Beach,-1,-1,,USD,"Junior Software and Data Engineer, Systematic Equities
Junior Software and Data Engineer for Systematic Equities
Please direct all resume submissions to QuantTalentUS@mlp.com and reference REQ-13151 in the subject.
Millennium is a top tier global hedge fund with a strong commitment to leveraging market innovations in technology and data to deliver high-quality returns.
A small, collaborative, entrepreneurial and highly pedigreed systematic investment team is seeking a Junior Software Engineer. This opportunity provides a dynamic and fast-paced environment with excellent opportunities for career growth.
Location
Irvine, CA
Principal Responsibilities
Help build out infrastructure to streamline the ingestion of data from a variety of sources
Design and maintain automated processes with intricate dependencies
Preferred Technical Skills
Bachelor or Master’s degree in Computer Science, Engineering, Applied Mathematics, Statistics or related STEM field
Strong programming skills in Python
Experience with shell scripts in a Linux environment
Advanced knowledge of SQL
Proficiency in math and probability
Preferred Experience
1-2 years of software engineering experience working with data
Strong communication and attention to detail
Demonstrated curiosity and desire to acquire new skill sets and tackle new problems
Highly Valued Relevant Experience:
Experience working with financial data
Practice working with data in some capacity
Highly Valued Technical Experience
Experience with C++
Target Start Date
As soon as possible
Please direct all resume submissions to QuantTalentUS@mlp.com and reference REQ-13151 in the subject.",Finanzas,,2022-08-21T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,Tupl,Texas,Dallas,-1,-1,,USD,"You will join our data engineering team and help Tupl to expand our big data automation platform within specific customer projects.

Dallas/USA
Full time

Tupl is a tech company that develops market leading solutions that bring innovation to the backend system of cellular networks and many other verticals. We have a strong knowledge on Wireless communications and IT technologies, especially AI and Machine Learning.
As part of our team, distributed between USA, Spain and Japan, you will have a unique opportunity to grow your professional career by helping us in the continued success of our business transformation software!
Your main responsibilities
Right now, Tupl is focused on creating use cases in the telecom segment, so telecom experience is always a plus. Below are the basic job duties for you:
Design and implement data collection components to ingest different data sources into big data systems
Design and implement ETL pipelines
Identify ways to improve data reliability, efficiency and quality
Collaborate effectively in cross-functional teams
Develop, integrate, deploy and test any Big Data tools and frameworks required by the customer projects
Contribute to team effort by accomplishing related results as needed
Close interaction with customer in order to identify project scope, functional requirements, and timelines
Required Technical Skills and QualificationsBachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field.Understanding of data modelling, algorithms, and data transformation techniques are the basics to work with data platforms.Understanding of data science concepts.Hands-on experience with ETL tools.BI tools knowledgeBig data technologies: Hadoop and Kafka.ML frameworks and libraries: TensorFlow, Spark, PyTorch, mlpack.Experience with Java, Python, R, C/C#, Golang.Teamwork and collaboration skills.Upper English Level
What you will love about Tupl!A competitive salary and benefits24 days of paid Time OffIRA with 3% matchingOpen and sharing environment where you will be exposed to bleeding edge technologiesInternational exposure and support to grow in your careerModern hardware like MacBook Pro and peripherals
Disclaimer
Tupl is committed to a diverse and inclusive workplace. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",,,2022-08-22T00:00:00,2022-10-16,FULL_TIME
Data Engineer Junior USA,Veridic Solutions,,,-1,-1,,USD,"Location-USA
Experience - 1 +
Role responsibilities:
Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology
Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes
Translate product backlog items into engineering designs and logical units of work
Profile and analyze data for the purpose of designing scalable solutions
Define and apply appropriate data acquisition and consumption strategies for given technical scenarios
Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem
Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns
Implement complex automated routines using workflow orchestration tools
Work with architecture, engineering leads and other teams to ensure quality solutions are implemented, and engineering best practices are defined and adhered to
Anticipate, identify and solve issues concerning data management to improve data quality
Build and incorporate automated unit tests and participate in integration testing efforts
Utilize and advance continuous integration and deployment frameworks
Troubleshoot data issues and perform root cause analysis
Work across teams to resolve operational & performance issues
The following qualifications and technical skills will position you well for this role:
MS/BS in Computer Science, or related technical discipline
5+ years of experience in large-scale software development, 3+ years of big data experience
Strong programming experience, Python preferred
Extensive experience working with Hadoop and related processing frameworks such as Spark, Hive, etc.
Experience with messaging/streaming/complex event processing tooling and frameworks with an emphasis on Spark Streaming or Structured Streaming and Apache Nifi
Good understanding of file formats including JSON, Parquet, Avro, and others
Familiarity with data warehousing, dimensional modeling, and ETL development
Experience with RDBMS systems, SQL and SQL Analytical functions
Experience with workflow orchestration tools like Apache Airflow
Familiarity with data warehousing, dimensional modeling, and ETL development
Experience with performance and scalability tuning
The following skills and experience are also relevant to our overall environment, and nice to have:
Experience with Scala or Java
Experience working in a public cloud environment, particularly AWS, and with services like EMR, S3, Lambda, Elastic ache, DynamoDB, SNS, SQS, etc
Familiarity with cloud warehouse tools like Snowflake
Experience building RESTful APIs to enable data consumption
Familiarity with build tools such as Terraform or CloudFormation and automation tools such as Jenkins or Circle CI
Familiarity with practices like Continuous Development, Continuous Integration and Automated Testing
Experience in Agile/Scrum application development",Tecnologías de la información,,2022-09-02T00:00:00,2022-10-16,FULL_TIME
Jr. Data Engineer,Canyon Associates,New Jersey,Parsippany,-1,-1,,USD,"Assist in the development of data assets, reports, queries and modelsKnowledge of database systems, excel, and BI toolsUnderstand Data Modeling conceptsCRM experience is a plusAzure is a plusCreate data assets, pipelines and reportsJob Type: Full-timePay: $90,000.00 - $115,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee discountFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveProfessional development assistanceVision insuranceSchedule:Monday to FridayCOVID-19 considerations:Must be vaccinatedEducation:Bachelor's (Preferred)Work Location: One location",,,2022-09-06T00:00:00,2022-10-16,FULL_TIME
"Cleared Software Engineer (Full Stack with any languages, Big Data Environment. Junior candidates can apply)",Leading Path Consulting,Virginia,Reston,-1,-1,,USD,"This position is funded and vacant and the customer is actively hiring for this role. Will consider Junior candidates!!!
Position is with the VA McLean Customer and requires an active TS/SCI with Full Scope Poly clearance.
The Software Engineer will design, develop, code, test, and debug complex new software products, or make significant
enhancements to existing software. The ideal candidate is a hands-on platform builder with experience in
developing scalable data platforms, with experience in business intelligence, analytics, data science and data products.
They must have firsthand technical expertise in a variety of configuration management and big data
technologies and the proven ability to fashion robust scalable solutions that can manage large data sets. They must be
at ease working in an agile environment with little supervision. This person should embody a passion for continuous
improvement and innovation.
Requirements
1. Demonstrated experience with software development in any language
2. Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the
equivalent combination of education, technical training, or work/military experience
Benefits
Leading Path is an award-winning Information Technology and Management Consulting firm focused on providing solutions in process, technology, and operations to our government and Fortune 500 clients. We offer a professional and supportive family-friendly work environment with a strong work-life balance. Leading Path provides a comprehensive and competitive benefits package, 401K, tuition reimbursement and opportunities for professional growth and advancement.",Tecnologías de la información,,2022-08-22T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,Open Road Media,New York State,New York,-1,-1,,USD,"About Open Road Integrated Media
Open Road Integrated Media is a prestige content brand delivering digital experiences that entertain and inform readers around the world. Open Road was founded in 2009 with the belief that great marketing and great content are the engines of growth for underserved authors and books. This philosophy is at the core of everything that we do. Open Road revolutionizes how publishers service authors, agents, and readers.
Summary
The Data Engineering and Analytics team is seeking a Junior Data Engineer. This role will work on various types of tasks and projects. The members of Data Engineering and Analytics team uses rigorous analytics to generate insights that inform product, marketing, and business decisions across the company. In the meantime, we build systems, infrastructure, data products for collecting, storing, and visualizing data to foster the data democracy in the company. We work in Python, SQL and we work with technologies like Airflow, Django, Tableau, Spark.
Essential Functions
ETL
Design and build ETL pipelines by using Airflow to collect data from different sources to data warehouses
Build pipeline integration with our various data products for long-running processes
Identify the room for optimizing relational data storage through design, query optimization, indices, replicas, partitioning, etc.
Automation
Build ad-hoc scripts or recurring processes to fully/partially automate labor-intensive workflows of other departments (e.g. Production, Marketing)
Requirements
1-2 years hands-on experience in ETL design, implementation and maintenance
Experience in schema design and data modeling
Experience in writing complex SQL queries to extract data from relational databases (e.g. MySQL, Redshift)
Experience in version control systems such as Git
Experience in the following tools/technology is a plus
Airflow, Django, Git, Docker
Comfortable with extensive Python coding
Willing to work with a codebase that is not originally written by you
Good communication skills; understand that being an effective engineer is about communicating with people as much as it is about writing code
Willing to learn any language/tools/frameworks that are necessary to get the job done

wqwv4PNMQK",Audiovisual y medios de comunicación,,2022-09-07T00:00:00,2022-10-16,FULL_TIME
Jr. Software Engineer - Big Data/Cloud,"Modern Technology Solutions, Inc.",Virginia,Chantilly,-1,-1,,USD,"Overview:

Position Overview:
Modern Technology Solutions, Inc. (MTSI) is seeking a Jr. Software Engineer. You will have the opportunity to grow with and support a small, highly empowered team at our Chantilly, VA location where every team member is directly responsible for the success. Your focus will be on projects emphasizing cloud-hosted solutions and services, Big Data technologies, and modern web frameworks all within an agile development model.
Why is MTSI known as a Great Place to Work?
Interesting Work: Our co-workers support some of the most important and critical programs to our national defense and security.
Values: Our first core value is that employees come first. We challenge our co-workers to provide the highest level of support and service, and reward them with some of the best benefits in the industry.
100% Employee Ownership: we have a stake in each other's success, and the success of our customers. It's also nice to know what's going on across the company; we have company wide town-hall meetings three times a year.
Great Benefits:
20 days PTO/year + 10 holidays/year
Flexible schedules
6% 401k match with immediate vesting
Semi-annual bonus eligibility (July and December)
Company funded Employee Stock Ownership Plan (ESOP) - a separate qualified retirement account
Up to $10,000 in annual tuition reimbursement
Other company funded benefits
Optional zero deductible Blue Cross/Blue Shield health insurance plan
Track Record of Success: We have grown every year since our founding in 1993
For additional company information, please visit: www.mtsi-va.com
Responsibilities:

As a Jr. Software Engineer with MTSI you will focus on projects emphasizing cloud-hosted solutions and services, data science and Data ingestion technologies, and modern web frameworks all within an agile development model. This position will be located in Chantilly, VA.
Your essential job functions will include but may not be limited to:
You will support work on large data sets and near-real-time content services and 2-D and 3-D visualization, using standards-based approaches.
You will supporting software design and development, as well as test and documentation activities.
Qualifications:
1+ years of software engineering, development, or Big Data experience, to include recent experience in agile software engineering environments.
TS clearance with SCI eligibility.
You must be able to demonstrate skill in in at least one or more high-level programming languages (e.g., Java, C/C++).
An interest and some knowledge of one modern Big Data (e.g., Hadoop), ETL (e.g., NiFi), Data Science (e.g., MapReduce), or web framework (e.g., AngularJS) technology.
You must be able to work both independently and in a collaborative team environment and meet required schedules and timelines.
You must have the ability to interact with data suppliers, work well with coworkers and teammates.
You must have outstanding skills in communicating complex technical issues and in providing comprehensive written, oral and/or digital products (including document organization and technical writing).
You must have good analytic skills and the ability to apply these skills in a multi-tasking environment where more than one project may require his/her participation at a given time (typically one primary project and one or two ancillary projects).
Desired Qualifications:
Specific experience with implementing and deploying applications and services on Commercial Cloud Services (C2S).
Experience with commercial cloud services, such as:
Amazon Web Services.
Experience with the Cloudera platform.
Experience with the following technologies:
RESTful Web Services.
Parallel programming.
Big Data experience with Accumulo, HBase or GeoMesa
Apache NiFi
Docker or Kubernetes
Experience with software design and implementation compliant with Risk Management Framework (RMF) requirements and ICD 503, including familiarity with IC PKI and Attribute Based Access Control.
Experience with National Reconnaissance Office (NRO) programs, or other IC software engineering projects.
Educational Requirements:
B.S. in a computer science, data science, engineering, math, statistics, operations research, or related field.
Certifications Desired:
AWS Certified
Security+ Certified
Clearance Requirements:
TS clearance with SCI eligibility.
U.S. Citizenship is required for this position.
#LI-DB1
#MTSIjobs
#mtsi
#100esop",Aeroespacial y defensa,,2022-08-23T00:00:00,2022-10-16,FULL_TIME
Junior Data Engineer,OneMain Financial,North Carolina,Charlotte,-1,-1,,USD,"One Main Financial is currently seeking qualified candidates for our IT Software Engineer position within our Credit Card group, located in Charlotte, NC.
The candidate should be able to adapt to a challenging fast paced environment working with batch and real-time systems involving customer credit card activities.
Requirements:
Strong analytical and problem solving skills
Strong desire to learn AWS cloud technologies, Python, and Spark to build resilient data pipelines
Write and maintain code.
Monitor the technical performance of internal systems.
Great communication skills, both written and verbal
A strong work ethic and ability to adapt quickly in a fast-paced environment
BS in Computer Science preferred
Additional qualifications (Nice to have):
Ability to write SQL
Knowledge in BigData processing (Hadoop Echosystem)
Note: Employment-based non-immigrant visa sponsorship and/or assistance is not offered for this specific job opportunity.
Benefits:
Because we want our team members to bring us their very best every day, we believe they deserve the right opportunities and benefits. That’s why we packed our comprehensive benefits package for full- and some part-timers with:
Health and wellbeing options for team members and their dependents
Up to 4% matching 401(k)
Tuition reimbursement
Continuing education
Bonus eligible
Paid time off
Paid volunteer time
And more
Our Company:
OneMain Financial is the country’s largest lending-exclusive financial company, a trusted name in lending for over 100 years. Since 2005 alone, we have looked beyond customers credit scores to lend more than $152 billion to 16.2 million people looking for simple, affordable loans.
With branches across 44 states, we're proud partners of the families and communities we serve. They turn to us to help meet important financial needs, including debt consolidation, medical expenses, household bills and auto purchases. It’s all about doing the right thing – a mission that hasn’t changed for more than 100 years.",Finanzas,,2022-09-10T00:00:00,2022-10-16,FULL_TIME
Jr/Sr Data Engineer,ROSEN Group,Ohio,Dublin,-1,-1,,USD,"New Challenge: New Team, New Office! Protect Our Environment By Your Algorithms
Did you ever think or did you hear about pipelines and integrity? Or, why should pipelines need machine learning?
The answer is simple: Pipeline inspection generates a lot of sensor data (e.g. from ultrasonic sensors) which have to be analyzed using state of the art computer vision techniques. Think of it like doing healthcare just for pipelines.
Interested? Then learn more about industrial machine learning applications and data analysis for pipeline data in a start-up environment. Be part of a new team, shape the team and your future and enjoy working in a multi-cultural and multi-national team.
RESPONSIBILTIES:
Design of a big data infrastructure and big data platform (storage, formats, interfaces, etc.)
Creation of ETL pipelines to validate, enrich and persist data
Provision of data sources for consumers in the form of interfaces (APIs)
Fusion of extremely structured, semi-structured and unstructured data sources
(e.g. historical data, metadata, simulation data, etc.)
Development or application of suitable tools for data access
Maintenance and continuous improvement of machine learning models
Definition of data requirements together with Data Scientists
Visit of national and international conferences on a regular basis
REQUIREMENTS:

To become part of the ROSEN family, you convince through a result-oriented, structured and independently working method. Moreover, you should bring with you:
Successfully completed a degree in computer science or other area with strong programming background
Mastering of a programming language like python and database systems
Good grasp of big data infrastructures and relevant big data tools
Passion for data and for the development of tools to master them
Good grasp of containerization technology and Linux
Knowledge of industry wide technology trends and best practices
Good teamwork skills in interdisciplinary teams

Qualification or Skills:

Education and Experience:
University degree in computer science, commercial information technology, physics or equivalent
OUR OFFER

ROSEN USA offers an exceptional working environment, salary commensurate with experience and incredible benefits package.","Servicios de construcción, reparación y mantenimiento",,2022-08-23T00:00:00,2022-10-16,OTHER
